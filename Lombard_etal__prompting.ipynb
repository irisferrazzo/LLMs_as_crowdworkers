{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Acting Like Humans? Evaluating Large Language Models as Proxies in Linguistic Experiments\n",
        "\n",
        "\n",
        "This notebook refers to the paper \"Acting Like Humans? Evaluating Large Language Models as Proxies in Linguistic Experiments\", which aims to replicate linguistic experimental pipelines with human participants using LLMs.\n",
        "\n",
        "\n",
        "It is intended to be used for further research.\n",
        "\n"
      ],
      "metadata": {
        "id": "9O2ZpQ84chnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code structure: #\n",
        "\n",
        "In the **first block of the code**, some libraries (such as OpenAI) are imported. They provide us with certain functions/applications that are already \"ready-to-use,\" so we don’t have to code them explicitly.\n",
        "\n",
        "Furthermore, the second cell is intended to be used as a test to ensure that everything has been imported correctly."
      ],
      "metadata": {
        "id": "DeGwUaQhWxOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ],
      "metadata": {
        "id": "6otEGts9Q6ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] =\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# prompt example, a test:\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\", \"content\": \"Say this is a test, it works\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "gpj-Y4VqO-Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the **second block** of the code, the functions for prompt engineering are written, which we will use for our analysis. Example functions include zero-shot and few-shot prompting. However, you are welcome to try other prompting techniques that we have explored in the seminar or that you have found online. This helps in conducting new and improved experiments."
      ],
      "metadata": {
        "id": "xyvUbo_iZ76e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Function to perform zero-shot prompting\n",
        "def zero_shot_prompting(task, prompt):\n",
        "    \"\"\"\n",
        "    Performs zero-shot prompting by sending a prompt to the language model\n",
        "    without providing any prior examples.\n",
        "\n",
        "    Args:\n",
        "    task (str): The task to be performed.\n",
        "    prompt (str): The text input sent to the language model.\n",
        "\n",
        "    Returns:\n",
        "    str: The response from the language model.\n",
        "    \"\"\"\n",
        "    # Create the chat message and send it to the language model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o4-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant\"},  # CHANGE HERE IF DESIRED\n",
        "            {\"role\": \"user\", \"content\": prompt}  # User message with the actual prompt\n",
        "        ]\n",
        "    )\n",
        "    # Return the content of the model’s first response message\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# Function to perform few-shot prompting\n",
        "def few_shot_prompting(task, examples, prompt):\n",
        "    \"\"\"\n",
        "    Performs few-shot prompting by providing some examples\n",
        "    before sending the actual prompt to the language model.\n",
        "\n",
        "    Args:\n",
        "    task (str): The task to be performed.\n",
        "    examples (list of dict): A list of examples, each example being a dictionary with 'input' and 'output'.\n",
        "    prompt (str): The text input sent to the language model.\n",
        "\n",
        "    Returns:\n",
        "    str: The response from the language model.\n",
        "    \"\"\"\n",
        "    # Initialize the messages list with a system message\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are an assistant\"}]  # CHANGE HERE IF DESIRED\n",
        "\n",
        "    # Add the examples to the messages\n",
        "    for example in examples:\n",
        "        messages.append({\"role\": \"user\", \"content\": example['input']})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": example['output']})\n",
        "\n",
        "    # Add the actual prompt to the messages list\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Create the chat message and send it to the language model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o4-mini\",\n",
        "        messages=messages\n",
        "    )\n",
        "    # Return the content of the model’s first response message\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "hjmUVUACL8wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replication pipeline step 1\n",
        "In the **third block** of the code you find code so that you can read in your uploaded data. This will be helpful in presenting the data to the LLM during prompting.\n",
        "\n",
        "Data handling code is provided for both replications. According to the data read in the cell, choose the corresponding passage and comment the other one out."
      ],
      "metadata": {
        "id": "OCAGXMgKadMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import gc\n",
        "from psutil import virtual_memory\n",
        "from datetime import datetime\n",
        "\n",
        "# Load data\n",
        "daten = 'Lombard_replicat_appendix.csv'  # Adjust the filename as needed\n",
        "df = pd.read_csv(daten)\n",
        "\n",
        "# Extract specific columns and pack them into a dictionary\n",
        "columns_to_extract = ['change', 'regularity', 'process', 'neologism', 'target_sent']\n",
        "selected_data = df[columns_to_extract].astype(str)\n",
        "stimuli_dict = selected_data.to_dict(orient=\"index\")\n",
        "\n",
        "# Create list of items\n",
        "all_items = [entry['target_sent'] for entry in stimuli_dict.values()]\n",
        "print(\"The materials for the study are:\")\n",
        "print(all_items)"
      ],
      "metadata": {
        "id": "5rDs7Gpugynz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replication pipeline step 2\n",
        "In the **fourth block** we can apply the prompt engineering functions of block 2. We will formulate our prompts in this cell. In this block, we can test different prompting strategies on one LLM-query and on a limited subset of the dataset."
      ],
      "metadata": {
        "id": "RVADzDhrcHx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To test one \"LLM-participant\" with a subset of the items\n",
        "\n",
        "# Preparing to store the model's responses\n",
        "answers_zero_shot = {}\n",
        "\n",
        "# === Zero-Shot Prompting ===\n",
        "print(\"=== Zero-Shot ===\")\n",
        "\n",
        "# Iterating through the subset for Zero-Shot\n",
        "for i, text in enumerate(all_items[:3]):\n",
        "    zero_shot_prompt = f\"Insert your instructions here: '{text}'\"\n",
        "\n",
        "    # Performing Zero-Shot Prompting\n",
        "    zero_shot = zero_shot_prompting(\"Task description\", zero_shot_prompt)\n",
        "\n",
        "    # Storing the response in the dictionary with the index as the key\n",
        "    answers_zero_shot[i] = {\n",
        "        \"Prompt\": zero_shot_prompt,\n",
        "        \"Response\": zero_shot\n",
        "    }\n",
        "\n",
        "    # Printing the input and the corresponding output\n",
        "    print(f\"Input: {text}\")\n",
        "    print(f\"Output: {zero_shot}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "RGt5oGzMVnrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the **fifth block** we save the results in an Excel file. This file stores the answers of ONE LLM-participant."
      ],
      "metadata": {
        "id": "rT3E2kchAorJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl.utils import get_column_letter\n",
        "from datetime import datetime\n",
        "import csv\n",
        "\n",
        "\n",
        "def convert_csv_to_excel(csv_path, excel_path):\n",
        "    \"\"\"\n",
        "    Konvertiert eine CSV-Datei in eine Excel-Datei (.xlsx).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.to_excel(excel_path, index=False)\n",
        "\n",
        "def load_or_create_excel(file_path):\n",
        "    \"\"\"\n",
        "    Lädt eine bestehende Excel-Datei oder erstellt eine neue, falls die Datei nicht existiert.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        workbook = openpyxl.load_workbook(file_path)\n",
        "        sheet = workbook.active\n",
        "    except FileNotFoundError:\n",
        "        workbook = openpyxl.Workbook()\n",
        "        sheet = workbook.active\n",
        "        print(f\"Datei '{file_path}' nicht gefunden. Das ist ein Fehler, schauen Sie den Namen der Datei, die im 3. Block hochgeladen wird\")\n",
        "    return workbook, sheet\n",
        "\n",
        "def add_columns_to_excel(sheet, new_columns):\n",
        "    \"\"\"\n",
        "    Fügt neue Spalten zur Excel-Datei hinzu.\n",
        "    \"\"\"\n",
        "    existing_columns = sheet.max_column\n",
        "    for idx, col_name in enumerate(new_columns, start=existing_columns + 1):\n",
        "        sheet[f\"{get_column_letter(idx)}1\"] = col_name\n",
        "\n",
        "def add_data_to_excel(sheet, data_dict, start_row):\n",
        "    \"\"\"\n",
        "    Fügt die Inhalte eines Dictionaries zur Excel-Tabelle hinzu.\n",
        "    \"\"\"\n",
        "    for idx, (key, entry) in enumerate(data_dict.items(), start=start_row):\n",
        "        sheet[f\"A{idx}\"] = key + 1\n",
        "        sheet[f\"B{idx}\"] = entry['Antwort']\n",
        "        sheet[f\"C{idx}\"] = entry['Prompt']\n",
        "        sheet[f\"D{idx}\"] = entry['Antwort']  # Antwort-Original\n",
        "        sheet[f\"E{idx}\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(\"Daten hinzugefügt.\")\n",
        "\n",
        "def save_excel(workbook, file_path):\n",
        "    \"\"\"\n",
        "    Speichert das Workbook in die angegebene Datei.\n",
        "    \"\"\"\n",
        "    workbook.save(file_path)\n",
        "    print(f\"Datei erfolgreich gespeichert unter: {file_path}\")\n",
        "\n",
        "def extend_csv(csv_input_path, csv_output_path, data_dict):\n",
        "    \"\"\"\n",
        "    Erweitert eine CSV-Datei direkt um neue Spalten und speichert sie als neue CSV-Datei.\n",
        "    \"\"\"\n",
        "    with open(csv_input_path, mode='r', newline='') as infile, open(csv_output_path, mode='w', newline='') as outfile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        fieldnames = reader.fieldnames + [\"Prompt\", \"Antwort_vom_Modell\", \"Datum\"]\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Wir gehen nur so weit wie das data_dict Einträge hat\n",
        "        for idx, row in enumerate(reader):\n",
        "            if idx < len(data_dict):  # Nur verarbeiten, wenn ein entsprechender Eintrag im Dictionary existiert\n",
        "                row[\"Prompt\"] = data_dict[idx][\"Prompt\"]\n",
        "                row[\"Antwort_vom_Modell\"] = data_dict[idx][\"Antwort\"]\n",
        "                row[\"Datum\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                writer.writerow(row)\n",
        "\n",
        "    print(f\"Erweiterte CSV-Datei gespeichert unter: {csv_output_path}\")\n",
        "\n",
        "\n",
        "# --- Parameter ---\n",
        "csv_file_path = daten\n",
        "excel_file_path = \"ergebnisse_ein_prob.xlsx\"\n",
        "csv_output_path = \"ergebnisse_ein_prob.csv\" # diese Datei kann auch hier auf Google Colab mit Doppelklick geöffnet werden\n",
        "\n",
        "# 1. Konvertiere CSV -> Excel und erweitere diese\n",
        "convert_csv_to_excel(csv_file_path, excel_file_path)\n",
        "workbook, sheet = load_or_create_excel(excel_file_path)\n",
        "add_columns_to_excel(sheet, [\"Prompt\", \"Antwort_vom_Modell\", \"Datum\"])\n",
        "add_data_to_excel(sheet, antworten_null_shot, start_row=sheet.max_row + 1) # HIER ÄNDERN: antworten_null_shot --> antworten_few_shot, etc.\n",
        "save_excel(workbook, excel_file_path)\n",
        "\n",
        "# 2. Alternativ: CSV direkt erweitern\n",
        "extend_csv(csv_file_path, csv_output_path, antworten_null_shot) # HIER ÄNDERN: antworten_null_shot --> antworten_few_shot, etc.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KnkgSGIIAnG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replication pipeline step 3\n",
        "In the **sixth block** we repeat the prompting that worked best for as many times as we have (or want to have) subjects and with all data in the corpus."
      ],
      "metadata": {
        "id": "6ohvcW9YI5O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "antworten_null_shot_all = {}\n",
        "probanden_zahl= 68\n",
        "reaction_time_per_participant=list()\n",
        "\n",
        "import time  # Import the time module\n",
        "\n",
        "for iteration in range(1, probanden_zahl + 1):  # Repeat the process\n",
        "    print(f\"### LLM-informant {iteration} ###\")\n",
        "\n",
        "    antworten_null_shot = {}  # Dictionary for null-shot responses in this iteration\n",
        "\n",
        "    # === Null-Shot ===\n",
        "    # Record the start time\n",
        "    start_time_pro_part = time.time()\n",
        "\n",
        "    for i, text in enumerate(all_items):\n",
        "        start_time = time.time()\n",
        "        zero_shot_prompt = f\"Vous êtes de langue maternelle française. Vous participez à une étude. Indiquez si la phrase suivante contient un mot nouveau ou un mot existant employé avec un sens nouveau simplement par 'oui' ou 'non': '{text}' Si ce mot existe, indiquez-le sans l'expliquer\"\n",
        "        zero_shot = zero_shot_prompting(\"choix binaire\", zero_shot_prompt)\n",
        "\n",
        "        # Record the end time and calculate the elapsed time for this sentence\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # Save the response in the dictionary with the index as the key\n",
        "        antworten_null_shot[i] = {\n",
        "            \"Prompt\": zero_shot_prompt,\n",
        "            \"Antwort\": zero_shot,\n",
        "            \"Zeit\": elapsed_time\n",
        "        }\n",
        "\n",
        "        #print(f\"Input: {text}\")\n",
        "        #print(f\"Output: {zero_shot}\")\n",
        "        print(i,f\"Time taken: {elapsed_time:.2f} seconds\")\n",
        "        print()\n",
        "\n",
        "    # Calculate elapsed time for the iteration\n",
        "    elapsed_time_pro_part = time.time() - start_time_pro_part\n",
        "\n",
        "    # Save the null-shot results of this iteration in the parent dictionary\n",
        "    antworten_null_shot_all[iteration] = antworten_null_shot\n",
        "    reaction_time_per_participant.append(elapsed_time_pro_part)\n",
        "    print(f\"Elapsed time for iteration {iteration}: {elapsed_time_pro_part:.2f} seconds\")\n",
        "    print()\n",
        "\n",
        "print(antworten_null_shot_all)\n",
        "print(\"Mean reaction time per participant:\", sum(reaction_time_per_participant)/len(reaction_time_per_participant))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QGrOTCQ-KKuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the **seventh block** we save the results from all test subjects in an Excel and csv file. These files subsequently store the answers of MULTIPLE test LLMs-participants."
      ],
      "metadata": {
        "id": "KQcBmTkKXcU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Dictionary to DataFrame conversion\n",
        "def dict_to_dataframe(antworten_dict):\n",
        "    rows = []\n",
        "    for iteration, prompts in antworten_dict.items():\n",
        "        for index, entry in prompts.items():\n",
        "            for val in stimuli_dict.values():\n",
        "                prompt_text = entry['Prompt']\n",
        "                #print(val['target_sent'].replace(\"'\",\"\"))\n",
        "                #print(prompt_text.split(\"'non':\")[1].split(\"Si\")[0].strip().replace(\"'\", \"\"))\n",
        "                if val['target_sent'].replace(\"'\",\"\").strip() == prompt_text.split(\"'non':\")[1].split(\"Si\")[0].strip().replace(\"'\", \"\"):\n",
        "                  #print(\"yes\",val['target_sent'].replace(\"'\",\"\").strip(),prompt_text.split(\"'non':\")[1].split(\"Si\")[0].strip().replace(\"'\", \"\"))\n",
        "                  rows.append({\n",
        "                        \"neologism\": val['neologism'],\n",
        "                        \"sentences\": val['target_sent'],\n",
        "                        \"change\": val['change'],\n",
        "                        \"regularity\": val['regularity'],\n",
        "                        \"process\": val['process'],\n",
        "                        \"Informant\": iteration,\n",
        "                        \"Index\": index,\n",
        "                        \"Prompt\": entry[\"Prompt\"],\n",
        "                        \"models_answer\": entry[\"Antwort\"],\n",
        "                        \"time\": entry[\"Zeit\"],\n",
        "                        \"date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    })\n",
        "                else:\n",
        "                  continue\n",
        "                  #print(\"no\",val['target_sent'].replace(\"'\",\"\").strip(),prompt_text.split(\"'non':\")[1].split(\"Si\")[0].strip().replace(\"'\", \"\"))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "new_data_df = dict_to_dataframe(antworten_null_shot_all)\n",
        "\n",
        "# Save the DataFrame as new files\n",
        "new_data_df.to_excel('3neolog_all_nullshot_o4mini.xlsx', index=False)\n",
        "new_data_df.to_csv('3neolog_all_nullshot_o4mini.csv', index=False)\n",
        "\n",
        "print(\"Data successfully saved!\")\n"
      ],
      "metadata": {
        "id": "WPznhbdvP7zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replication pipeline step 4\n",
        "Finally, we evaluate the results."
      ],
      "metadata": {
        "id": "pa5KZ_s1RbkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval neologisms\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Load results\n",
        "results_file = 'neolog_CoT_all.csv.csv'\n",
        "\n",
        "# Counters\n",
        "correct_oui = 0\n",
        "correct_neo = 0\n",
        "fillers = 0\n",
        "fillers_wrong = 0\n",
        "mistakes_no_neo = {}\n",
        "mistakes_no_oui = {}\n",
        "filler_mistakes = {}\n",
        "filler_error_list = []\n",
        "without_neo = 0\n",
        "wrong_neo = 0\n",
        "mistakes_no_oui_list=[]\n",
        "wrong_neo_after_oui = 0\n",
        "\n",
        "# Helper functions\n",
        "def clean_answer(answer):\n",
        "    \"\"\"Normalize whitespace, remove commas and colons, and clean up the answer string.\"\"\"\n",
        "    answer = answer.lower().strip()\n",
        "    answer = answer.replace(',', ' ')  # Replace commas with spaces\n",
        "    answer = answer.replace(':', ' ')  # Replace colons with spaces\n",
        "    answer = re.sub(r'\\s+', ' ', answer)  # Collapse multiple spaces/newlines/tabs into one space\n",
        "    return answer\n",
        "\n",
        "def extract_word(answer):\n",
        "    \"\"\"Extract the word after 'oui'.\"\"\"\n",
        "    parts = answer.split()\n",
        "    if len(parts) > 1:\n",
        "        return parts[1]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "correct_neo_conditions = {\n",
        "    ('Morphological', 'Irregular'): 0,\n",
        "    ('Morphological', 'Regular'): 0,\n",
        "    ('Semantic', 'Irregular'): 0,\n",
        "    ('Semantic', 'Regular'): 0,\n",
        "}\n",
        "\n",
        "# Read CSV\n",
        "rows=[]\n",
        "with open(results_file, mode='r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    header = next(csv_reader, None)  # Skip header if present\n",
        "\n",
        "    for row in csv_reader:\n",
        "        neologism = row[0].lower().strip()\n",
        "        rows.append(neologism)\n",
        "        answer = clean_answer(row[8])\n",
        "\n",
        "        if row[2] != \"Filler\":\n",
        "            if answer.startswith(\"oui\"):\n",
        "              neo = extract_word(answer)\n",
        "\n",
        "              if neo:\n",
        "                  if neo.startswith(neologism[:-3]):\n",
        "                      correct_oui += 1\n",
        "                      correct_neo += 1\n",
        "                      condition_key = (row[2].strip(), row[3].strip())\n",
        "                      if condition_key in correct_neo_conditions:\n",
        "                          correct_neo_conditions[condition_key] += 1\n",
        "                      else:\n",
        "                          print(\"Unexpected condition key:\", condition_key)\n",
        "                  else:\n",
        "                      wrong_neo += 1\n",
        "                      wrong_neo_after_oui += 1\n",
        "                      print(\"wrong neo\", neologism, neo)\n",
        "              else:\n",
        "                  correct_oui += 1  # Accept if \"oui\" without any neo (e.g., \"oui.\" or \"oui :)\")\n",
        "                  without_neo += 1\n",
        "                  key = (row[5], row[6])\n",
        "                  mistakes_no_neo[key] = (row[0].strip(), row[2].strip(), row[3].strip(), row[4].strip())\n",
        "\n",
        "            else:\n",
        "              key = (row[5], row[6])\n",
        "              mistakes_no_oui[key] = (row[0].strip(), row[2].strip(), row[3].strip(), row[4].strip())\n",
        "              mistakes_no_oui_list.append(answer)\n",
        "\n",
        "        else:\n",
        "            fillers += 1\n",
        "            if answer.startswith(\"oui\"):\n",
        "                fillers_wrong += 1\n",
        "                filler_mistakes.setdefault(row[1], answer)\n",
        "                filler_error_list.append(row[1])\n",
        "\n",
        "# fillers\n",
        "wrong_fillers_counts = dict(Counter(filler_error_list))\n",
        "print(\"Wrong fillers:\", fillers_wrong)\n",
        "print(\"Wrong filler counts:\", wrong_fillers_counts)\n",
        "\n",
        "# Note: You have 'reaction_time_per_participant' in your original script,\n",
        "# but it is not defined anywhere. Commenting it out for now.\n",
        "\n",
        "# whole_duration = sum(reaction_time_per_participant)\n",
        "# print(\"Mean reaction time per participant:\", sum(reaction_time_per_participant) / len(reaction_time_per_participant))\n",
        "# print(\"Whole study duration:\", whole_duration)\n",
        "\n",
        "print(\"Total responses:\", len(rows))\n",
        "print(\"Correct 'oui' responses:\", correct_oui)\n",
        "print(\"Correct neologism guesses:\", correct_neo)\n",
        "print(\"Correct but without neo\", without_neo)\n",
        "print(\"Wrong neologisms\", wrong_neo)\n",
        "print(\"Fillers:\", fillers)\n",
        "print(\"Mistakes --> critical answers without oui\", len(mistakes_no_oui_list))\n",
        "\n",
        "# Error analysis\n",
        "morph_reg = 0\n",
        "morph_irreg = 0\n",
        "sem_reg = 0\n",
        "sem_irreg = 0\n",
        "\n",
        "print()\n",
        "print(\"mistakes_no_neo\",mistakes_no_neo)\n",
        "for val in mistakes_no_neo.values():\n",
        "    task_type, reg_type = val[1], val[2]\n",
        "    if task_type == 'Morphological' and reg_type == 'Irregular':\n",
        "        morph_reg += 1\n",
        "    elif task_type == 'Morphological' and reg_type == 'Regular':\n",
        "        morph_irreg += 1\n",
        "    elif task_type == 'Semantic' and reg_type == 'Irregular':\n",
        "        sem_reg += 1\n",
        "    elif task_type == 'Semantic' and reg_type == 'Regular':\n",
        "        sem_irreg += 1\n",
        "    else:\n",
        "        print(\"Unknown type:\", val)\n",
        "\n",
        "wrong_target_words = [val[0] for val in mistakes_no_neo.values()]\n",
        "mistakes_counts = dict(Counter(wrong_target_words))\n",
        "'''print(\"Mistakes no neo:\", mistakes_no_neo)\n",
        "print(\"Morphological regular mistakes:\", morph_irreg)\n",
        "print(\"Morphological irregular mistakes:\", morph_reg)\n",
        "print(\"Semantic regular mistakes:\", sem_irreg)\n",
        "print(\"Semantic irregular mistakes:\", sem_reg)\n",
        "print(\"Wrong target words frequency:\", mistakes_counts)\n",
        "val_mistakes_counts=list(mistakes_counts.values())\n",
        "print(\"sum mistakes no neo:\", sum(val_mistakes_counts))'''\n",
        "\n",
        "morph_reg = 0\n",
        "morph_irreg = 0\n",
        "sem_reg = 0\n",
        "sem_irreg = 0\n",
        "print()\n",
        "for val in mistakes_no_oui.values():\n",
        "    task_type, reg_type = val[1], val[2]\n",
        "    if task_type == 'Morphological' and reg_type == 'Irregular':\n",
        "        morph_reg += 1\n",
        "    elif task_type == 'Morphological' and reg_type == 'Regular':\n",
        "        morph_irreg += 1\n",
        "    elif task_type == 'Semantic' and reg_type == 'Irregular':\n",
        "        sem_reg += 1\n",
        "    elif task_type == 'Semantic' and reg_type == 'Regular':\n",
        "        sem_irreg += 1\n",
        "    else:\n",
        "        print(\"Unknown type:\", val)\n",
        "\n",
        "'''\n",
        "print(\"Mistakes no oui:\", mistakes_no_oui)\n",
        "print(\"Morphological regular mistakes:\", morph_irreg)\n",
        "print(\"Morphological irregular mistakes:\", morph_reg)\n",
        "print(\"Semantic regular mistakes:\", sem_irreg)\n",
        "print(\"Semantic irregular mistakes:\", sem_reg)'''\n",
        "\n",
        "########################################\n",
        "\n",
        "# Total expected positives (non-filler responses)\n",
        "expected_positives = len(rows) - fillers\n",
        "\n",
        "# General mistakes: answers not starting with \"oui\" when they should\n",
        "general_mistakes = len(mistakes_no_oui_list)\n",
        "correct_general = expected_positives - general_mistakes\n",
        "accuracy_general = correct_general / expected_positives\n",
        "\n",
        "'''print(\"\\nCorrect 'oui' + correct neologism by condition:\")\n",
        "for cond, count in correct_neo_conditions.items():\n",
        "    print(f\"{cond[0]} {cond[1]}: {count}\")'''\n",
        "\n",
        "# Neologism mistakes: answered \"oui\" but did not provide correct neologism\n",
        "correct_neo_plus_oui = correct_neo  # already defined in your original script\n",
        "accuracy_neo = correct_neo_plus_oui / expected_positives\n",
        "\n",
        "# Correct 'oui' responses (may or may not include correct neologism)\n",
        "accuracy_oui_only = correct_oui / expected_positives\n",
        "# Percentage of 'oui' answers with wrong neologisms\n",
        "percent_wrong_neo_after_oui = wrong_neo_after_oui / correct_oui if correct_oui else 0\n",
        "\n",
        "print(\"\\n===== ACCURACY REPORT =====\")\n",
        "print(f\"Expected positive answers (non-fillers): {expected_positives}\")\n",
        "print(f\"Correct answers with 'oui' (excluding wrong neologisms): {correct_oui}\")\n",
        "print(f\"Correct answers with 'oui' + correct neologism: {correct_neo_plus_oui}\")\n",
        "print(f\"Mistakes (no 'oui' when expected): {general_mistakes}\")\n",
        "print(f\"Accuracy (general 'oui' recognition): {accuracy_general:.2%}\")\n",
        "print(f\"Accuracy (exact match with neologism): {accuracy_neo:.2%}\")\n",
        "print()\n",
        "print(f\"Percentage of 'oui' answers with wrong neologism: {percent_wrong_neo_after_oui:.2%}\")\n",
        "\n",
        "\n",
        "# Breakdown by condition\n",
        "condition_counts = {\n",
        "    ('Morphological', 'Irregular'): {'total': 0, 'correct_oui': 0, 'correct_neo': 0},\n",
        "    ('Morphological', 'Regular'): {'total': 0, 'correct_oui': 0, 'correct_neo': 0},\n",
        "    ('Semantic', 'Irregular'): {'total': 0, 'correct_oui': 0, 'correct_neo': 0},\n",
        "    ('Semantic', 'Regular'): {'total': 0, 'correct_oui': 0, 'correct_neo': 0}\n",
        "}\n",
        "\n",
        "# Reload and reprocess to count per condition\n",
        "with open(results_file, mode='r', encoding='utf-8') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    header = next(csv_reader, None)\n",
        "\n",
        "    for row in csv_reader:\n",
        "        if row[2] == \"Filler\":\n",
        "            continue\n",
        "\n",
        "        task_type = row[2].strip()\n",
        "        reg_type = row[3].strip()\n",
        "        condition = (task_type, reg_type)\n",
        "        answer = clean_answer(row[8])\n",
        "        target = row[0].strip().lower()\n",
        "        neo = extract_word(answer)\n",
        "\n",
        "        if condition not in condition_counts:\n",
        "            print(\"Warning: Unknown condition:\", condition)\n",
        "            continue\n",
        "\n",
        "        condition_counts[condition]['total'] += 1\n",
        "\n",
        "        if answer.startswith(\"oui\"):\n",
        "            condition_counts[condition]['correct_oui'] += 1\n",
        "\n",
        "            if neo and neo.startswith(target[:-3]):\n",
        "                condition_counts[condition]['correct_neo'] += 1\n",
        "            #else:\n",
        "              #print(\"no neo\", answer)\n",
        "\n",
        "# Print condition breakdown\n",
        "print(\"\\n===== CONDITION BREAKDOWN =====\")\n",
        "for condition, stats in condition_counts.items():\n",
        "    total = stats['total']\n",
        "    oui = stats['correct_oui']\n",
        "    neo = stats['correct_neo']\n",
        "    acc_oui = oui / total if total else 0\n",
        "    acc_neo = neo / total if total else 0\n",
        "    print(f\"{condition[0]} - {condition[1]}:\")\n",
        "    print(f\"  Total: {total}\")\n",
        "    print(f\"  'Oui' correct: {oui} ({acc_oui:.2%})\")\n",
        "    print(f\"  Neologism correct (both no neo and wrong neo): {neo} ({acc_neo:.2%})\")\n",
        "\n",
        "accuracy_fillers = 1 - (fillers_wrong / fillers) if fillers else 0\n",
        "print(f\"\\nAccuracy on fillers: {accuracy_fillers:.2%}\")"
      ],
      "metadata": {
        "id": "-1tOx3fz6kRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval neologisms old\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# Results hochladen\n",
        "results= 'all_neolog_CoT_o4-mini.csv'\n",
        "\n",
        "correct_oui = 0\n",
        "correct_neo = 0\n",
        "fillers = 0\n",
        "fillers_wrong = 0\n",
        "mistakes = {}\n",
        "filler_mistakes = {}\n",
        "filler_error_list=[]\n",
        "\n",
        "# Open the CSV file\n",
        "with open(results, mode='r', encoding='utf-8') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "\n",
        "    # Optional: Get the header if the file has one\n",
        "    header = next(csv_reader, None)  # Skip the header if present\n",
        "\n",
        "    # Access and process each line\n",
        "    for row in csv_reader:\n",
        "        #print(row)\n",
        "        neologism = row[0].lower().strip()\n",
        "        answer = row[8].lower().strip()\n",
        "        #print(answer)\n",
        "\n",
        "        if row[2] != \"Filler\":\n",
        "          #print(answer)\n",
        "          if answer.startswith(\"oui\"):\n",
        "            correct_oui +=1\n",
        "            if len(answer)>3:\n",
        "              try:\n",
        "                neo = answer.split(\" \")[1].strip()\n",
        "              except IndexError:\n",
        "                continue\n",
        "\n",
        "              #print(neologism, neo)\n",
        "              if neo.startswith(neologism):\n",
        "                correct_neo +=1\n",
        "\n",
        "          else:\n",
        "            key= row[5], row[6]\n",
        "            mistakes[key] = row[0].strip(),row[2].strip(),row[3].strip(),row[4].strip()\n",
        "\n",
        "        elif row[2] == \"Filler\":\n",
        "          fillers +=1\n",
        "          if answer.startswith(\"oui\"):\n",
        "            fillers_wrong +=1\n",
        "            if row[1] not in filler_mistakes.keys():\n",
        "              filler_mistakes[row[1]] = [answer]\n",
        "            else:\n",
        "              filler_mistakes[row[1]].append(answer)\n",
        "\n",
        "            filler_error_list.append(row[1])\n",
        "\n",
        "        else:\n",
        "          print(row[1], answer)\n",
        "\n",
        "wrong_fillers_counts= dict(Counter(filler_error_list))\n",
        "wrong_target_words = []\n",
        "for m in mistakes.values():\n",
        "  wrong_target_words.append(m[0])\n",
        "mistakes_counts = dict(Counter(wrong_target_words))\n",
        "\n",
        "print(\"wrong fillers\",fillers_wrong)\n",
        "print(wrong_fillers_counts)\n",
        "\n",
        "\n",
        "whole_duration=sum(reaction_time_per_participant)\n",
        "print(\"tot responses\", correct_oui+len(mistakes))\n",
        "print(\"correct\",correct_oui)\n",
        "print(\"fillers\", fillers) #40\n",
        "print(\"mistakes\",len(mistakes), \":\", mistakes)\n",
        "#print(\"wrong target words\",set(wrong_target_words))\n",
        "print(\"wrong target words freq\",mistakes_counts)\n",
        "print(\"mean reaction time per participant:\", sum(reaction_time_per_participant)/len(reaction_time_per_participant))\n",
        "print(\"whole study duration:\", whole_duration)\n",
        "\n",
        "# error analysis\n",
        "morph_reg=0\n",
        "morph_irreg=0\n",
        "sem_reg=0\n",
        "sem_irreg=0\n",
        "for val in mistakes.values():\n",
        "  #print(val)\n",
        "  if val[1] == 'Morphological' and val[2] == 'Irregular':\n",
        "      morph_reg +=1\n",
        "  elif val[1] == 'Morphological' and val[2] == 'Regular':\n",
        "      morph_irreg +=1\n",
        "  elif val[1] == 'Semantic' and val[2] == 'Irregular':\n",
        "      sem_reg +=1\n",
        "  elif val[1] == 'Semantic' and val[2] == 'Regular':\n",
        "      sem_irreg +=1\n",
        "  else:\n",
        "    print(val)\n",
        "\n",
        "print(\"Morphological irregular\", morph_irreg)\n",
        "print(\"Morphological regular\", morph_reg)\n",
        "print(\"Semantic irregular\", sem_irreg)\n",
        "print(\"Semantic regular\", sem_reg)"
      ],
      "metadata": {
        "id": "aMfdapi5kkNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Für Python-Anfänger:**\n",
        "- schauen Sie gerne auf Google, falls Sie eine oder eine andere Aufgabe nicht lösen können. Im Gegensatz zur geisteswissenschaftlichen Arbeit ist Google beim Programmieren immer unser Freund. Bei Fehlermeldungen kann man den ausgegebenen Text copy pasten und in Google die Bedeutung suchen.\n",
        "- wenn Sie Teile vom Code nicht 100% greifen können, können Sie erstmals versuchen, ChatGPT zu prompten und es fragen, Ihnen die Inhalte für Programmieren-Laien zu erklären.\n",
        "- Am Anfang fühlt sich wahrscheinlich alles schwer/unklar an. Bitte melden Sie sich bei mir oder kommen Sie in meine Sprechstunde. Ich freue mich, Ihnen helfen zu können! Peer reviews/Feedback ist auch super :)"
      ],
      "metadata": {
        "id": "viJBkiqseHiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Paar **Infos zum Start**:\n",
        "\n",
        "* Alles, was durch das Symbol \"#\" vorausgesetzt wird, ist ein Kommentar. Das wird vom Programm nicht gelesen.\n",
        "\n",
        "* Wenn wir längere Abschnitte für das Programm auskommentieren wollen, können wir \"\"\"text\"\"\" (also \" x3) oder '''text''' (also ' x3) benutzen. Diese Option streckt sich auch über verschiedene Zeilen.\n",
        "\n",
        "* Einrücken (der leere Platz am Anfang einiger Zeilen, auf der Tastatur einmal «Tab» oder viermal «Leertaste» drücken) spielt eine grundlegende Rolle in Python. Einrücken = eine Gruppierung/hierarchische Beziehung zwischen der ersten Zeile und den unterligenden Zeilen. https://www.youtube.com/watch?v=m2UDI5Fy6qw\n",
        "\n",
        "* Um sicher zu gehen, dass das Programm wirklich macht, was wir wollen, kann man sich zwischendurch das Resultat der ausgeführten Aufgabe ausgeben lassen. Dafür benutzt man print(was man printen möchte). Print kann man auch zum Hinzufügen von eigenen Kommentaren benutzt werde, die zur Veranschauulichung von \"was gerade im System verarbeitet wird\" dienen.\n",
        "\n",
        "* Eine Variabel (wie in Mathe) ist ein Begriff, der eigentlich Platzhalter für etwas anders ist (siehe das kommende Beispiel, in dem text, text_split1 etc. Variabeln sind und im Code dann für alles stehen, was rechts der Gleichung stehen)\n",
        "\n",
        " - eine Variabel schreibt man so: text= \"blablabla\"\n",
        "\n",
        " - ein string schreibt man so: *\"text\"* oder 'text'\n",
        "\n",
        " - print(text) --> blablabla\n",
        "\n",
        " - print(\"text\") --> text\n",
        "\n",
        "* Manchmal sind die benutzten libraries (wie z.B. nltk, matplotlib, numpy usw.) noch nicht installiert. Das macht man so: !pip install name_der_library. Wenn das nicht funktioniert ist der Name der library vielleicht falsch geschrieben. Einfach googeln!\n",
        "\n",
        "\n",
        "\n",
        "Kurzes Cheat Sheet: https://www.mintpepper.ch/local/workshopDateien/beilage-produktentwicklung-software.pdf\n"
      ],
      "metadata": {
        "id": "MrAxM8Waeukw"
      }
    }
  ]
}