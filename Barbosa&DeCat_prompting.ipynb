{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O2ZpQ84chnQ"
      },
      "source": [
        "# Acting Like Humans? Evaluating Large Language Models as Proxies in Linguistic Experiments\n",
        "\n",
        "\n",
        "This notebook refers to the paper \"Acting Like Humans? Evaluating Large Language Models as Proxies in Linguistic Experiments\", which aims to replicate linguistic experimental pipelines with human participants using LLMs.\n",
        "\n",
        "\n",
        "It is intended to be used for further research.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeGwUaQhWxOM"
      },
      "source": [
        "# Code structure: #\n",
        "\n",
        "In the **first block** of the code, some libraries (such as OpenAI) are imported. They provide us with certain functions/applications that are already \"ready-to-use,\" so we don’t have to code them explicitly.\n",
        "\n",
        "Furthermore, the second cell is intended to be used as a test to ensure that everything has been imported correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6otEGts9Q6ua"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpj-Y4VqO-Jc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] =\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# prompt example, a test:\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\", \"content\": \"Say this is a test, it works\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"o4-mini\",\n",
        ")\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyvUbo_iZ76e"
      },
      "source": [
        "In the **second block** of the code, the functions for prompt engineering are written, which we will use for our analysis. Example functions include zero-shot and few-shot prompting. However, you are welcome to try other prompting techniques that we have explored in the seminar or that you have found online. This helps in conducting new and improved experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjmUVUACL8wu"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Function to perform zero-shot prompting\n",
        "def zero_shot_prompting(task, prompt):\n",
        "    \"\"\"\n",
        "    Performs zero-shot prompting by sending a prompt to the language model\n",
        "    without providing any prior examples.\n",
        "\n",
        "    Args:\n",
        "    task (str): The task to be performed.\n",
        "    prompt (str): The text input sent to the language model.\n",
        "\n",
        "    Returns:\n",
        "    str: The response from the language model.\n",
        "    \"\"\"\n",
        "    # Create the chat message and send it to the language model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o4-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant\"},  # CHANGE HERE IF DESIRED\n",
        "            {\"role\": \"user\", \"content\": prompt}  # User message with the actual prompt\n",
        "        ]\n",
        "    )\n",
        "    # Return the content of the model’s first response message\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# Function to perform few-shot prompting\n",
        "def few_shot_prompting(task, examples, prompt):\n",
        "    \"\"\"\n",
        "    Performs few-shot prompting by providing some examples\n",
        "    before sending the actual prompt to the language model.\n",
        "\n",
        "    Args:\n",
        "    task (str): The task to be performed.\n",
        "    examples (list of dict): A list of examples, each example being a dictionary with 'input' and 'output'.\n",
        "    prompt (str): The text input sent to the language model.\n",
        "\n",
        "    Returns:\n",
        "    str: The response from the language model.\n",
        "    \"\"\"\n",
        "    # Initialize the messages list with a system message\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are an assistant\"}]  # CHANGE HERE IF DESIRED\n",
        "\n",
        "    # Add the examples to the messages\n",
        "    for example in examples:\n",
        "        messages.append({\"role\": \"user\", \"content\": example['input']})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": example['output']})\n",
        "\n",
        "    # Add the actual prompt to the messages list\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Create the chat message and send it to the language model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o4-mini\",\n",
        "        messages=messages\n",
        "    )\n",
        "    # Return the content of the model’s first response message\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCAGXMgKadMV"
      },
      "source": [
        "# Replication pipeline step 1\n",
        "In the **third block** of the code you find code so that you can read in your uploaded data. This will be helpful in presenting the data to the LLM during prompting.\n",
        "\n",
        "Data handling code is provided for both replications. According to the data read in the cell, choose the corresponding passage and comment the other one out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rDs7Gpugynz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import gc\n",
        "from psutil import virtual_memory\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "# Load data\n",
        "daten = 'Barbosa&Cat_2019.csv'  # Adjust the filename as needed\n",
        "df = pd.read_csv(daten)\n",
        "\n",
        "# Extract specific columns and pack them into a dictionary\n",
        "columns_to_extract = ['French Sentences', 'syntactic position', 'trace position', 'status', 'gold_ratings']\n",
        "selected_data = df[columns_to_extract].astype(str)\n",
        "stimuli_dict = selected_data.to_dict(orient=\"index\")\n",
        "\n",
        "# Create list of items\n",
        "all_items = [entry['French Sentences'] for entry in stimuli_dict.values()]\n",
        "random.shuffle(all_items)\n",
        "print(\"The materials for the study are:\")\n",
        "print(all_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVADzDhrcHx-"
      },
      "source": [
        "# Replication pipeline step 2\n",
        "In the **fourth block** we can apply the prompt engineering functions of block 2. We will formulate our prompts in this cell. In this block, we can test different prompting strategies on one LLM-query and on a limited subset of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGt5oGzMVnrD"
      },
      "outputs": [],
      "source": [
        "# To test one \"LLM-participant\" with a subset of the items\n",
        "\n",
        "# Preparing to store the model's responses\n",
        "answers_zero_shot = {}\n",
        "\n",
        "# === Zero-Shot Prompting ===\n",
        "print(\"=== Zero-Shot ===\")\n",
        "\n",
        "# Iterating through the subset for Zero-Shot\n",
        "for i, text in enumerate(all_items[:3]):\n",
        "    zero_shot_prompt = f\"Insert your instructions here: '{text}'\"\n",
        "\n",
        "    # Performing Zero-Shot Prompting\n",
        "    zero_shot = zero_shot_prompting(\"Task description\", zero_shot_prompt)\n",
        "\n",
        "    # Storing the response in the dictionary with the index as the key\n",
        "    answers_zero_shot[i] = {\n",
        "        \"Prompt\": zero_shot_prompt,\n",
        "        \"Response\": zero_shot\n",
        "    }\n",
        "\n",
        "    # Printing the input and the corresponding output\n",
        "    print(f\"Input: {text}\")\n",
        "    print(f\"Output: {zero_shot}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT3E2kchAorJ"
      },
      "source": [
        "In the **fifth block** we save the results in an Excel file. This file stores the answers of ONE LLM-participant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnkgSGIIAnG9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl.utils import get_column_letter\n",
        "from datetime import datetime\n",
        "import csv\n",
        "\n",
        "\n",
        "def convert_csv_to_excel(csv_path, excel_path):\n",
        "    \"\"\"\n",
        "    Konvertiert eine CSV-Datei in eine Excel-Datei (.xlsx).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.to_excel(excel_path, index=False)\n",
        "\n",
        "def load_or_create_excel(file_path):\n",
        "    \"\"\"\n",
        "    Lädt eine bestehende Excel-Datei oder erstellt eine neue, falls die Datei nicht existiert.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        workbook = openpyxl.load_workbook(file_path)\n",
        "        sheet = workbook.active\n",
        "    except FileNotFoundError:\n",
        "        workbook = openpyxl.Workbook()\n",
        "        sheet = workbook.active\n",
        "        print(f\"Datei '{file_path}' nicht gefunden. Das ist ein Fehler, schauen Sie den Namen der Datei, die im 3. Block hochgeladen wird\")\n",
        "    return workbook, sheet\n",
        "\n",
        "def add_columns_to_excel(sheet, new_columns):\n",
        "    \"\"\"\n",
        "    Fügt neue Spalten zur Excel-Datei hinzu.\n",
        "    \"\"\"\n",
        "    existing_columns = sheet.max_column\n",
        "    for idx, col_name in enumerate(new_columns, start=existing_columns + 1):\n",
        "        sheet[f\"{get_column_letter(idx)}1\"] = col_name\n",
        "\n",
        "def add_data_to_excel(sheet, data_dict, start_row):\n",
        "    \"\"\"\n",
        "    Fügt die Inhalte eines Dictionaries zur Excel-Tabelle hinzu.\n",
        "    \"\"\"\n",
        "    for idx, (key, entry) in enumerate(data_dict.items(), start=start_row):\n",
        "        sheet[f\"A{idx}\"] = key + 1\n",
        "        sheet[f\"B{idx}\"] = entry['Antwort']\n",
        "        sheet[f\"C{idx}\"] = entry['Prompt']\n",
        "        sheet[f\"D{idx}\"] = entry['Antwort']  # Antwort-Original\n",
        "        sheet[f\"E{idx}\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(\"Daten hinzugefügt.\")\n",
        "\n",
        "def save_excel(workbook, file_path):\n",
        "    \"\"\"\n",
        "    Speichert das Workbook in die angegebene Datei.\n",
        "    \"\"\"\n",
        "    workbook.save(file_path)\n",
        "    print(f\"Datei erfolgreich gespeichert unter: {file_path}\")\n",
        "\n",
        "def extend_csv(csv_input_path, csv_output_path, data_dict):\n",
        "    \"\"\"\n",
        "    Erweitert eine CSV-Datei direkt um neue Spalten und speichert sie als neue CSV-Datei.\n",
        "    \"\"\"\n",
        "    with open(csv_input_path, mode='r', newline='') as infile, open(csv_output_path, mode='w', newline='') as outfile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        fieldnames = reader.fieldnames + [\"Prompt\", \"Antwort_vom_Modell\", \"Datum\"]\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Wir gehen nur so weit wie das data_dict Einträge hat\n",
        "        for idx, row in enumerate(reader):\n",
        "            if idx < len(data_dict):  # Nur verarbeiten, wenn ein entsprechender Eintrag im Dictionary existiert\n",
        "                row[\"Prompt\"] = data_dict[idx][\"Prompt\"]\n",
        "                row[\"Antwort_vom_Modell\"] = data_dict[idx][\"Antwort\"]\n",
        "                row[\"Datum\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                writer.writerow(row)\n",
        "\n",
        "    print(f\"Erweiterte CSV-Datei gespeichert unter: {csv_output_path}\")\n",
        "\n",
        "\n",
        "# --- Parameter ---\n",
        "csv_file_path = daten\n",
        "excel_file_path = \"ergebnisse_ein_prob.xlsx\"\n",
        "csv_output_path = \"ergebnisse_ein_prob.csv\" # diese Datei kann auch hier auf Google Colab mit Doppelklick geöffnet werden\n",
        "\n",
        "# 1. Konvertiere CSV -> Excel und erweitere diese\n",
        "convert_csv_to_excel(csv_file_path, excel_file_path)\n",
        "workbook, sheet = load_or_create_excel(excel_file_path)\n",
        "add_columns_to_excel(sheet, [\"Prompt\", \"Antwort_vom_Modell\", \"Datum\"])\n",
        "add_data_to_excel(sheet, antworten_null_shot, start_row=sheet.max_row + 1) # HIER ÄNDERN: antworten_null_shot --> antworten_few_shot, etc.\n",
        "save_excel(workbook, excel_file_path)\n",
        "\n",
        "# 2. Alternativ: CSV direkt erweitern\n",
        "extend_csv(csv_file_path, csv_output_path, antworten_null_shot) # HIER ÄNDERN: antworten_null_shot --> antworten_few_shot, etc.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ohvcW9YI5O2"
      },
      "source": [
        "# Replication pipeline step 3\n",
        "In the **sixth block** we repeat the prompting that worked best for as many times as we have (or want to have) subjects and with all data in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGrOTCQ-KKuN"
      },
      "outputs": [],
      "source": [
        "antworten_null_shot_all = {}\n",
        "probanden_zahl= 60\n",
        "reaction_time_per_participant=list()\n",
        "\n",
        "import time  # Import the time module\n",
        "\n",
        "for iteration in range(1, probanden_zahl + 1):  # Repeat the process\n",
        "    print(f\"### LLM-informant {iteration} ###\")\n",
        "\n",
        "    antworten_null_shot = {}  # Dictionary for null-shot responses in this iteration\n",
        "\n",
        "    # === Null-Shot ===\n",
        "    # Record the start time\n",
        "    start_time_pro_part = time.time()\n",
        "\n",
        "    for i, text in enumerate(all_items):\n",
        "        start_time = time.time()\n",
        "        zero_shot_prompt = f\"Vous participez à une étude. Vous êtes de langue maternelle française. Tache: Vous allez maintenant voir deux phrases. La phrase a. constitue le contexte, la phrase b. est la phrase que vous devez juger, pas la phrase a. Vous devez exprimer votre jugement seulement sur la phrase b. en utilisant le schéma suivant: a. Je pourrais dire cela. b. Je pourrais dire cela, mais dans un autre contexte. c. Je ne pourrais pas dire cela, mais je connais des gens qui pourraient le faire. d. Personne ne dirait cela. e. Je ne sais pas. Vous donnez seulement la lettre, pas la justification. Penser: Vous lisez le contenu (phrase a), puis vous vous concentrez sur la phrase b. Vous la lisez attentivement et vous vous demandez si cette formulation est acceptable. Vous vous posez la question: Est-ce que je, ou quelqu’un d’autre, pourrais dire cela ? Votre réponse vous aide à choisir la lettre qui exprimera votre jugement en la tache. Voici les deux phrases: '{text}' Réponse:\"\n",
        "        zero_shot = zero_shot_prompting(\"jugement d'acceptabilité\", zero_shot_prompt)\n",
        "\n",
        "        # Record the end time and calculate the elapsed time for this sentence\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # Save the response in the dictionary with the index as the key\n",
        "        antworten_null_shot[i] = {\n",
        "            \"Prompt\": zero_shot_prompt,\n",
        "            \"Antwort\": zero_shot,\n",
        "            \"Zeit\": elapsed_time\n",
        "        }\n",
        "\n",
        "        #print(f\"Input: {text}\")\n",
        "        print(f\"Output: {zero_shot}\")\n",
        "        print(i,f\"Time taken: {elapsed_time:.2f} seconds\")\n",
        "        print()\n",
        "\n",
        "    # Calculate elapsed time for the iteration\n",
        "    elapsed_time_pro_part = time.time() - start_time_pro_part\n",
        "\n",
        "    # Save the null-shot results of this iteration in the parent dictionary\n",
        "    antworten_null_shot_all[iteration] = antworten_null_shot\n",
        "    reaction_time_per_participant.append(elapsed_time_pro_part)\n",
        "    print(f\"Elapsed time for iteration {iteration}: {elapsed_time_pro_part:.2f} seconds\")\n",
        "    print()\n",
        "\n",
        "print(antworten_null_shot_all)\n",
        "print(\"Mean reaction time per participant:\", sum(reaction_time_per_participant)/len(reaction_time_per_participant))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQcBmTkKXcU8"
      },
      "source": [
        "In the **seventh block** we save the results from all test subjects in an Excel and csv file. These files subsequently store the answers of MULTIPLE test LLMs-participants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPznhbdvP7zc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "columns_to_extract = ['French Sentences', 'syntactic position', 'trace position', 'status', 'gold_ratings']\n",
        "# Dictionary to DataFrame conversion\n",
        "def dict_to_dataframe(antworten_dict):\n",
        "    rows = []\n",
        "    for iteration, prompts in antworten_dict.items():\n",
        "        for index, entry in prompts.items():\n",
        "            for val in stimuli_dict.values():\n",
        "                prompt_text = entry['Prompt']\n",
        "                #print(val['French Sentences'])\n",
        "                #print(prompt_text.split(\"deux phrases:\")[1].strip().replace(\"'\", \"\"))\n",
        "                prompt_cot=prompt_text.split(\"deux phrases:\")[1].strip().replace(\"'\", \"\")\n",
        "                #print(prompt_cot.split(\"Réponse\")[0].strip())\n",
        "                if val['French Sentences'].strip() == prompt_cot.split(\"Réponse\")[0].strip():\n",
        "                  #print(\"yes\")\n",
        "                  rows.append({\n",
        "                        \"French Sentences\": val['French Sentences'],\n",
        "                        \"syntactic position\": val['syntactic position'],\n",
        "                        \"trace position\": val['trace position'],\n",
        "                        \"status\": val['status'],\n",
        "                        \"gold_ratings\": val['gold_ratings'],\n",
        "                        \"Informant\": iteration,\n",
        "                        \"Index\": index,\n",
        "                        \"Prompt\": entry[\"Prompt\"],\n",
        "                        \"models_answer\": entry[\"Antwort\"],\n",
        "                        \"time\": entry[\"Zeit\"],\n",
        "                        \"date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    })\n",
        "                else:\n",
        "                  continue\n",
        "                  #print(\"no\",val['target_sent'].replace(\"'\",\"\").strip(),prompt_text.split(\"'non':\")[1].split(\"Si\")[0].strip().replace(\"'\", \"\"))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "new_data_df = dict_to_dataframe(antworten_null_shot_all)\n",
        "\n",
        "# Save the DataFrame as new files\n",
        "new_data_df.to_excel('all_accept_cotshot_gpt-4o-mini.xlsx', index=False)\n",
        "new_data_df.to_csv('all_accept_cotshot_gpt-4o-mini.csv', index=False)\n",
        "\n",
        "print(\"Data successfully saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replication pipeline step 4\n",
        "Finally, we evaluate the results."
      ],
      "metadata": {
        "id": "ztseP2QYRUZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import string\n",
        "import unicodedata\n",
        "\n",
        "results = 'all_accept_cotshot_o4-mini.csv'\n",
        "experimental_file = 'experimental_items_X1_to_X68.csv'\n",
        "\n",
        "sent_gold_ratings = {}\n",
        "ratings_per_sentence = defaultdict(list)\n",
        "status_dict = {}\n",
        "syntactic_dict = {}\n",
        "trace_dict = {}\n",
        "label_dict = {}\n",
        "grade_map = {'a': 5, 'b': 3, 'c': 1, 'd': -5, 'e': 0}\n",
        "translated_scores = defaultdict(list)\n",
        "rating_diff_data = []\n",
        "critical_sentence_set = set()  # Track sentences with \"critical\" in column 6 (index 6)\n",
        "baseline_sentence_set = set()\n",
        "\n",
        "def normalize(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
        "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Step 1: Read gold labels\n",
        "\n",
        "with open(experimental_file, mode='r', encoding='utf-8') as label_file:\n",
        "    label_reader = csv.reader(label_file)\n",
        "    next(label_reader)\n",
        "    for row in label_reader:\n",
        "        sentence_id = normalize(row[0])\n",
        "        sentence_text = normalize(row[1])\n",
        "        label_dict[sentence_text] = sentence_id\n",
        "        status = row[6].strip().lower()  # Column 7 contains 'critical' or 'baseline'\n",
        "\n",
        "        if status == 'critical':\n",
        "            critical_sentence_set.add(sentence_text)\n",
        "        elif status == 'baseline':\n",
        "            baseline_sentence_set.add(sentence_text)\n",
        "\n",
        "# Step 2: Read model results\n",
        "\n",
        "with open(results, mode='r', encoding='utf-8') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    next(csv_reader)\n",
        "\n",
        "    for row in csv_reader:\n",
        "        sentence = normalize(row[0])\n",
        "        syntactic_position = row[1]\n",
        "        trace_position = row[2]\n",
        "        status = row[3]\n",
        "        gold_rating = row[4]\n",
        "        model_answer = row[8]\n",
        "\n",
        "        sent_gold_ratings[sentence] = gold_rating\n",
        "        ratings_per_sentence[sentence].append({\n",
        "            'answer': model_answer,\n",
        "            'gold': gold_rating,\n",
        "            'status': status,\n",
        "            'syntactic': syntactic_position,\n",
        "            'trace': trace_position\n",
        "        })\n",
        "\n",
        "        status_dict[sentence] = status\n",
        "        syntactic_dict[sentence] = syntactic_position\n",
        "        trace_dict[sentence] = trace_position\n",
        "\n",
        "# Step 3: Score comparison and collect rating differences\n",
        "model_ratings_letter_all=[]\n",
        "model_ratings_letter_critical=[]\n",
        "model_ratings_letter_baseline=[]\n",
        "for sentence, answers in ratings_per_sentence.items():\n",
        "    model_ratings = []\n",
        "    gold_ratings = []\n",
        "    model_ratings_permodel = []\n",
        "\n",
        "    for entry in answers:\n",
        "        raw = entry['answer']\n",
        "        cleaned = normalize(raw)\n",
        "        if cleaned:\n",
        "            cleaned = cleaned[0]\n",
        "            model_ratings_letter_all.extend(cleaned)\n",
        "            model_rating = grade_map.get(cleaned, None)\n",
        "            if model_rating is not None:\n",
        "                gold_rating = float(sent_gold_ratings.get(sentence, 0))\n",
        "                model_ratings.append(model_rating)\n",
        "                gold_ratings.append(gold_rating)\n",
        "\n",
        "        if  entry['status'] == 'critical':\n",
        "          model_ratings_letter_critical.extend(cleaned)\n",
        "        elif  entry['status'] == 'baseline':\n",
        "          model_ratings_letter_baseline.extend(cleaned)\n",
        "\n",
        "    if model_ratings and gold_ratings:\n",
        "        mean_model_rating = np.mean(model_ratings)\n",
        "        mean_gold_rating = np.mean(gold_ratings)\n",
        "        rating_diff_abs = abs(mean_model_rating - mean_gold_rating)\n",
        "        rating_diff_signed = mean_model_rating - mean_gold_rating\n",
        "        label = label_dict.get(sentence, 'unknown')\n",
        "        status = answers[0]['status'] if answers else 'unknown'\n",
        "        rating_diff_data.append({\n",
        "            'sentence': sentence,\n",
        "            'label': label,\n",
        "            'mean_model': mean_model_rating,\n",
        "            'mean_gold': mean_gold_rating,\n",
        "            'rating_diff_abs': rating_diff_abs,\n",
        "            'rating_diff_signed': rating_diff_signed,\n",
        "            'status': status\n",
        "        })\n",
        "\n",
        "print(\"here!!!\",rating_diff_data)\n",
        "# Step 4: Check for sentences missing in model results\n",
        "missing_model_sentences = set(label_dict.keys()) - set(ratings_per_sentence.keys())\n",
        "print(f\"Sentences in gold data but missing in model results: {len(missing_model_sentences)}\")\n",
        "for s in missing_model_sentences:\n",
        "    print(f\"Missing: {s}\")\n",
        "\n",
        "# Step 5: Summary stats\n",
        "max_rating_diff_entry = max(rating_diff_data, key=lambda x: x['rating_diff_abs']) if rating_diff_data else None\n",
        "count_large_diffs_4 = (\n",
        "    sum(1 for item in rating_diff_data if item['rating_diff_abs'] >= 4),\n",
        "    [item['label'] for item in rating_diff_data if item['rating_diff_abs'] >= 4]\n",
        ")\n",
        "count_large_diffs_2 = sum(1 for item in rating_diff_data if item['rating_diff_abs'] >= 2)\n",
        "\n",
        "# Step 6: Aggregate rating differences by condition\n",
        "def rating_diff_stats_by_condition(attribute_dict):\n",
        "    grouped = defaultdict(list)\n",
        "    model_ratings_group = defaultdict(list)\n",
        "    gold_ratings_group = defaultdict(list)\n",
        "\n",
        "    for item in rating_diff_data:\n",
        "        sentence = item['sentence']\n",
        "        key = attribute_dict.get(sentence, 'unknown')\n",
        "        grouped[key].append(item['rating_diff_abs'])\n",
        "        model_ratings_group[key].append(item['mean_model'])\n",
        "        gold_ratings_group[key].append(item['mean_gold'])\n",
        "\n",
        "    stats = {}\n",
        "    for group in grouped:\n",
        "        mismatches = grouped[group]\n",
        "        model_vals = model_ratings_group[group]\n",
        "        gold_vals = gold_ratings_group[group]\n",
        "\n",
        "        mean_model = round(np.mean(model_vals), 2)\n",
        "        mean_gold = round(np.mean(gold_vals), 2)\n",
        "        mean_diff = round(abs(mean_model - mean_gold), 2)\n",
        "\n",
        "        stats[group] = {\n",
        "            'count': len(mismatches),\n",
        "            'mean_rating_diff_abs': round(np.mean(mismatches), 2),\n",
        "            'std_rating_diff': round(np.std(mismatches), 2),\n",
        "            'mean_model': mean_model,\n",
        "            'mean_gold': mean_gold,\n",
        "            'mean_model_vs_gold_diff': mean_diff\n",
        "        }\n",
        "\n",
        "    return stats\n",
        "\n",
        "status_rating_diff_stats = rating_diff_stats_by_condition(status_dict)\n",
        "syntactic_rating_diff_stats = rating_diff_stats_by_condition(syntactic_dict)\n",
        "trace_rating_diff_stats = rating_diff_stats_by_condition(trace_dict)\n",
        "label_rating_diff_stats = rating_diff_stats_by_condition(label_dict)\n",
        "\n",
        "# Step 7: Print summary\n",
        "print(\"\\n--- Global Summary ---\")\n",
        "print(\"Total sentences analyzed:\", len(rating_diff_data))\n",
        "print(f\"Max Rating Gap: Sentence '{max_rating_diff_entry['sentence']}' → Model: {max_rating_diff_entry['mean_model']}, Gold: {max_rating_diff_entry['mean_gold']}, Gap: {max_rating_diff_entry['rating_diff_abs']:.2f}\")\n",
        "print(f\"Count of Rating Gaps ≥ 4: {count_large_diffs_4}\")\n",
        "print(f\"Count of Rating Gaps ≥ 2: {count_large_diffs_2}\")\n",
        "\n",
        "def print_rating_diff_stats(name, stats):\n",
        "    print(f\"\\n{name} Rating Difference Stats:\")\n",
        "    for key, val in sorted(stats.items()):\n",
        "        print(f\"{key:<20} → gap: {val['mean_rating_diff_abs']:.2f}±{val['std_rating_diff']:.2f}, \"\n",
        "              f\"model: {val['mean_model']}, gold: {val['mean_gold']}, \"\n",
        "              f\"|Δ model-gold|: {val['mean_model_vs_gold_diff']}, n={val['count']}\")\n",
        "\n",
        "print_rating_diff_stats(\"Status\", status_rating_diff_stats)\n",
        "print_rating_diff_stats(\"Syntactic Position\", syntactic_rating_diff_stats)\n",
        "print_rating_diff_stats(\"Trace Position\", trace_rating_diff_stats)\n",
        "print_rating_diff_stats(\"Label\", label_rating_diff_stats)\n",
        "\n",
        "binned_rating_diffs_all = {\n",
        "    \"0-1\": 0,\n",
        "    \"1-2\": 0,\n",
        "    \"2-3\": 0,\n",
        "    \"3-4\": 0,\n",
        "    \">4\": 0\n",
        "}\n",
        "\n",
        "binned_rating_diffs_critical = {\n",
        "    \"0-1\": 0,\n",
        "    \"1-2\": 0,\n",
        "    \"2-3\": 0,\n",
        "    \"3-4\": 0,\n",
        "    \">4\": 0\n",
        "}\n",
        "\n",
        "binned_rating_diffs_baseline = {\n",
        "    \"0-1\": 0,\n",
        "    \"1-2\": 0,\n",
        "    \"2-3\": 0,\n",
        "    \"3-4\": 0,\n",
        "    \">4\": 0\n",
        "}\n",
        "\n",
        "# Separate the data into critical and baseline\n",
        "critical_rating_diff_data = []\n",
        "baseline_rating_diff_data = []\n",
        "\n",
        "for entry in rating_diff_data:\n",
        "    sentence = entry['sentence']\n",
        "    if sentence in critical_sentence_set:\n",
        "        critical_rating_diff_data.append(entry)\n",
        "    elif sentence in baseline_sentence_set:\n",
        "        baseline_rating_diff_data.append(entry)\n",
        "\n",
        "# Now, we can print the results for both categories:\n",
        "print(\"ALL rating diff data count:\", len(rating_diff_data))\n",
        "print(\"Critical rating diff data count:\", len(critical_rating_diff_data))\n",
        "print(\"Baseline rating diff data count:\", len(baseline_rating_diff_data))\n",
        "\n",
        "# Bin the rating differences for critical sentences\n",
        "for item in rating_diff_data:\n",
        "    rating_diff = item['rating_diff_abs']\n",
        "    #print(rating_diff)\n",
        "    if rating_diff < 1:\n",
        "        binned_rating_diffs_all[\"0-1\"] += 1\n",
        "    elif rating_diff < 2:\n",
        "        binned_rating_diffs_all[\"1-2\"] += 1\n",
        "    elif rating_diff < 3:\n",
        "        binned_rating_diffs_all[\"2-3\"] += 1\n",
        "    elif rating_diff < 4:\n",
        "        binned_rating_diffs_all[\"3-4\"] += 1\n",
        "    else:\n",
        "        binned_rating_diffs_all[\">4\"] += 1\n",
        "\n",
        "# Bin the rating differences for critical sentences\n",
        "for item in critical_rating_diff_data:\n",
        "    rating_diff = item['rating_diff_abs']\n",
        "    #print(rating_diff)\n",
        "    if rating_diff < 1:\n",
        "        binned_rating_diffs_critical[\"0-1\"] += 1\n",
        "    elif rating_diff < 2:\n",
        "        binned_rating_diffs_critical[\"1-2\"] += 1\n",
        "    elif rating_diff < 3:\n",
        "        binned_rating_diffs_critical[\"2-3\"] += 1\n",
        "    elif rating_diff < 4:\n",
        "        binned_rating_diffs_critical[\"3-4\"] += 1\n",
        "    else:\n",
        "        binned_rating_diffs_critical[\">4\"] += 1\n",
        "\n",
        "# Bin the rating differences for baseline sentences\n",
        "for item in baseline_rating_diff_data:\n",
        "    rating_diff = item['rating_diff_abs']\n",
        "    #print(rating_diff)\n",
        "    if rating_diff < 1:\n",
        "        binned_rating_diffs_baseline[\"0-1\"] += 1\n",
        "    elif rating_diff < 2:\n",
        "        binned_rating_diffs_baseline[\"1-2\"] += 1\n",
        "    elif rating_diff < 3:\n",
        "        binned_rating_diffs_baseline[\"2-3\"] += 1\n",
        "    elif rating_diff < 4:\n",
        "        binned_rating_diffs_baseline[\"3-4\"] += 1\n",
        "    else:\n",
        "        binned_rating_diffs_baseline[\">4\"] += 1\n",
        "\n",
        "# Print the results for all sentences\n",
        "total_all = len(rating_diff_data)\n",
        "print(\"\\n--- Binned Rating Differences ALL ---\")\n",
        "for bin_label, count in binned_rating_diffs_all.items():\n",
        "    percentage = (count / total_all * 100) if total_all else 0\n",
        "    print(f\"{bin_label} → {count} sentences ({percentage:.2f}%)\")\n",
        "\n",
        "# Print the results for critical sentences\n",
        "total_critical = len(critical_rating_diff_data)\n",
        "print(\"\\n--- Binned Rating Differences (Critical only) ---\")\n",
        "for bin_label, count in binned_rating_diffs_critical.items():\n",
        "    percentage = (count / total_critical * 100) if total_critical else 0\n",
        "    print(f\"{bin_label} → {count} sentences ({percentage:.2f}%)\")\n",
        "\n",
        "# Print the results for baseline sentences\n",
        "total_baseline = len(baseline_rating_diff_data)\n",
        "print(\"\\n--- Binned Rating Differences (Baseline only) ---\")\n",
        "for bin_label, count in binned_rating_diffs_baseline.items():\n",
        "    percentage = (count / total_baseline * 100) if total_baseline else 0\n",
        "    print(f\"{bin_label} → {count} sentences ({percentage:.2f}%)\")\n",
        "\n",
        "# Step 5: Model Rating Usage (Critical vs Baseline)\n",
        "grade_letters = ['a', 'b', 'c', 'd', 'e']\n",
        "\n",
        "def print_grade_distribution(rating_list, label):\n",
        "    counter = Counter(rating_list)\n",
        "    total = sum(counter.values())\n",
        "\n",
        "    print(f\"\\nGrade Distribution for {label}:\")\n",
        "    for grade in grade_letters:\n",
        "        count = counter.get(grade, 0)\n",
        "        percentage = (count / total * 100) if total > 0 else 0\n",
        "        print(f\"{grade.upper()} → {count} times ({percentage:.2f}%)\")\n",
        "\n",
        "print_grade_distribution(model_ratings_letter_all, \"All\")\n",
        "print_grade_distribution(model_ratings_letter_critical, \"Critical\")\n",
        "print_grade_distribution(model_ratings_letter_baseline, \"Baseline\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_g6kwbJ5yfda"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}